# data-analysis-foundation
## データ分析基盤入門メモ
データ種類
構造データ・・・リレーショナルデータベース(RDB)といったスキーマが決まっているデータ
半構造データ・・JSON、CSVなど異なるプログラミング言語間でデータをやり取りするもの
構造データ・・・ExcelやPDF、画像など決まった構造（スキーマ）を持っていないデータ


半構造データと構造データの違いはデータの型が決まっているかどうか（前者が決まっていないのに対し、後者は決まっている。）

ノード…サーバーの表す単位のこと

データ分析基盤の変遷
シングルノード…サーバー一台（パソコン一台のイメージ）
↓
マルチノード、クラスター…複数ノードで一つの目的を達成するために集められた集合体（パソコン複数台で一気に処理を行うイメージ）
マルチノードのメインプロダクト
Apache Hadoop
Apache Spark
Apache Hive
Apache Kafka(Google Cloud Pub/SubやAmazon Kinesisの先駆け）
↓
クラウド時代(2010年代後半〜）
デカップリング（decoupling)…データ分析におけるストレージ機能と計算処理機能を分離可能に
デカップリングで計算能力やストレージ機能を無限にアウトスケールできるようになる
クラウド関連の主要なサービス
GCP..Google Cloud Platform
AWS…Amazon Web Service AZURE…Microsoft Azure
共通した機能
・データを保存するサービス
・データを処理するサービス
・メタデータと連携するサービス
・SQLでデータを参照するサービス

処理速度のボトルネックを考える時に出現するサービス
・シェアードエブリシング…スモールデータで利用される。短い時間で大量のトランザクションを捌くことに特化しているアーキテクチャ
・シェアードナッシング…分散型のローカルストレージを用いて運用され、数百TBを超える大容量のデータ保存と、故障への耐性を高めた構成。複数のプロセスが競合した場合に、リソースの取り合いが発生、処理の遅延
・シェアードオンリーデータ…AmazonS3やGoogle Cloud Strageといったスケール可能なクラウドストレージに配置を行い、計算リソースはAmazon EMR,Amazon Redshift、Google BigQueryといったプロダクトからデータが利用されるが、計算リソースはユーザーの用途に応じて好きなものからアクセスできるという点。

データ分析基盤の持つ役割
データレイク….データレイクとはローデータやそれに近い形のデータをそのまま保存する場所
データウェアハウス…構造化されたデータを保持する役割を持つ領域
データマート…データが加工されている状態

分散処理の登場
スレッド処理…シングルノードで行われる分散処理。一台のノードのスペックを上げることでしかスケールが不可能。webサイトの処理やメッセージキューからデータをサブスクライブする時などに使う。
分散処理…複数のノードを用いて処理を行う。ノードを増やせば増やす分だけ無限にスケールアウト可能

Hdoop…分散処理のかつてのメインサービス。ビッグデータを扱う際に複数の並列処理を可能とするエコシステム。
MPPDB….通常のMySQLやPostgreSQLといったスモールデータシステムで利用される技術要素をそのまま分散処理に対応させたもの

オンプレミス（英：on-premises）とはクラウドじゃない方。もう少し具体的に書くとコンピュータやシステムを自分たちのところに設置して自分たちで運用すること。


データのサイロ化…各部門が独立して業務自体が完結してしまっており、お互いの間に壁が存在してしまっている状態。サイロ化が発生すると、データを取り込むために同一の手法が利用できず、特殊な設定やバッチを何個も作成しなければならない。

データエンジニアのスキルセット
データエンジニア…データを集めて、統合して分析をサポートする仕事。分散システムの構築管理、データの取り込みやETLを通したデータパイプラインの最適化、ユーザーへのアクセス環境の提供、データが格納されているストレージの管理などを行う（データエンジニアリング）

データサイエンティスト…コンピューターサイエンスや数学のバックグラウンドがあることが多く、機械学習やディープラーニングの知見を持っており、PythonやJavaを使いこなす人

データアナリスト…社内におけるビジネスプロセスや手続きに詳しいプロフェッショナル。ビジネスに対する答えを持っている人。

ビジネスアナリスト…BIツールなどを駆使して分析を行なっていく人のこと。

オペレーションエンジニア…データ分析基盤の業務プロセスを改善し、最適化するための存在

データパイプライン…データの物流のようなもので、データソースからデータが分析者の手元に届くまでのデータの流れを指す。
データソース→データレイク→データウェアハウス→データマート

アドホック分析…定期的に行われるデータ分析や、項目も内容も決まっているデータ分析と違って、その都度単発的に行われるというのが特徴。

Quality beats Quantity(量より質）

DataOps…データ収集からアナリティクスを通した価値提供（デリバリー）の速度を最大限にすること
セルフサービスモデル…データエンジニアがデータ基盤の調整から開発まで全てを請け負うのではなく、エンジニアによってあらかじめ用意された適切な設備（インタフェース）を利用して自分自身でデータを利用し、価値を見出す仕事

データ基盤の構成はシンプルイズベスト

データ分析基盤の基本構造
コレクティングレイヤー…データを集めるためのインターフェースの集まり。RDB、支払い表のエクセルデータなど
データ収集には、絶え間なく収集を行うストリーミング、一定以上の塊のデータを収集するバッチ処理、ひとまず仮のデータを配置するプロビジョニングの三つがある。

ストレージレイヤ-….データやメタデータを保存するレイヤーのこと

プロセシングレイヤー…保存されたデータやメタデータに対して「関係」と「パターン」を見つけるために操作を行うレイヤーのこと。ETLやデータラングリング、暗号化、データ品質計算/メタデータ計算などを行う

アクセスレイヤー….データ分析基盤とユーザーとの接点を持つレイヤーのこと。GUI、BIツール、APIやストレージへの直接アクセス、メッセージキューに対するアクセスなどでデータにアクセスするレイヤー。

メッセージキュー…ストリーミングデータを帆zんするために利用する。異なるアプリケーションプログラム間で動作を連携させてデータを交換させる際の方式のひとつで、送るデータをキューと呼ばれるデータ領域に保持し、データを受ける側の処理が完了するのを待たずに次の処理へ移る方式のこと。

プロセシングレイヤー
ETL(Extract transform load)…データを整形してより分析向けの形にしたり、精度の高いデータを生成する行為を指す。ETLの技術として、バッチ処理においてはApache HiveやApache Sparkなどが使われます。

データラングリング…非構造データを構造データにしたり、付加価値をつける作業を指す。データラングリングにはデータストラクチャリング、データクレンジング、データエンリッチングの三つがある。
データストラクチャリング…非構造データを構造データにするような作業を指す
データクレンジング…データに含まれた重複したデータ、壊れているデータ、特定のフォーマットに沿っていないデータを取り除くこと。
データエンリッチング…データクレンジングしたデータに対して、分析に必要な情報を付加する作業。例えば、同一ユーザーと思われる人へ付与するセッション情報など（ログインIDとパスワード）

ETLとデータラングリングの違いはETLがより正規のワークフローとして定義され、ITのプロフェッショナルによって構築/管理/最適化される一方で、データラングリングは原則IT部署のプロフェッショナルが担う作業ではないこと。

暗号化
トランスペアレントエンクリプション…データがディスクに書き込まれるときに暗号化され、読み出されるときに複合化される仕組み
エクスプリシットエンクリプション…データを全く使えないものに暗号化してしまうこと
ディアイデンティフィケーション…データの特性を残しつつ、個人を特定しにくいようにする手法。コーホート(cohorts)パターンとサブトラクトパターンがある。
コーホートパターンとはお互いに性質の近いデータを入れ替えてしまう手法。サブトラクトは特定の情報を引き算するという意味、

データプロファイリング…データに関する情報のサマリーを取得する。
データ品質の測定やメタデータの計算はApache Spark向けのDeeequライブラリなどを使ってデータ品質やメタデータ取得処理を自前で実装することが多い。

ストレージレイヤー
マスターデータ…部署コードや商品コードなど、データエンリッチングやデータマートを作成する際に特に重要なデータ。マスターデータ管理にはデータ活用型など様々な手法がある。

データ活用型マスター管理…データ分析基盤以外のシステムでマスターデータを生成し、それらを集約し統合することでデータ分析基盤としてのマスターを作成する方法。データ活用を行う前提で周りのシステムを設計しているケースは稀。データを活用するために各システムからデータを取り込んで、データ分析基盤の中でデータ分析基盤用のマスターデータを作成し、データ分析基盤処理では作成したマスターデータを利用するのがデータ活用するのがマスターデータ管理

データのライフサイクル管理
データのライフサイクルとはデータがデータソースから発生してから、削除またはアーカイブされるまでのデータの流れを指す。データが発生してからデータが利用されるまでの流れは整理されているケースが多いが、保存されているデータが役目を終えた時のことが忘れられている場合が多い、

メタデータ…データについてのデータ。あるデータそのものではなく、そのデータを表す属性や関連する情報を記述したデータのこと。データを効率的に管理したり検索したりするためには、メタデータの適切な付与と維持が重要となる。
メタデータの種類
・ビジネスメタデータ（テーブル定義やドメイン知識）
・テクニカルメタデータ（ログデータや技術詳細、データ品質、データプロファイリング情報）
・オペレーショナルメタデータ（操作履歴など）

データのゾーン管理
ローゾーン…集めたデータをそのまま保存しておく場所
ゴールドゾーン…データ分析基盤における主要なゾーンで、データマートやデータウェアハウスの役割を果たす。
ステージングゾーン…普遍なデータの提供が役割。理論的にこのデータを保持しておけば、大惨事がったとしても、ゴールドゾーンのデータを修復することができる。
クォレンティーゾーン…機密情報を保持する隔離されたゾーン。ここのデータを参照する人は限られていることが大きが、データ分析において価値が高い情報があるため許可制で利用することがある。
テンポラリーゾーン…プロヴィ序ニングによって配置されたデータを格納するゾーン。ユーザーが気が向いたときに好きなデータや、対象のデータのオーナーに声をかけてデータを取り込む。

アクセスレイヤー
SQLを経由して使うような事前にスキーマを定義しておくことによって使う方式をスキーマオンライトと呼ぶ。
Apache Sparkなどのプロダクトはスキーマオンライトだけでなく、スキーマオンリードと呼ばれる。（ParquetやArvoはファイルの内部にスキーマ情報を保持しており、その情報を利用することで実現する。）

ファイル連携…データ分析基盤をスモールデータシステムやサードパーティツールと絵rん系する場合は、素朴な方法になりますが、ファイルによる連携が行われる。ファイル連携にはストレージレイヤーのファイルをコピーや編集して渡す、ストレージレイヤーのファイルを直接参照させるといった二つの方法がある。

クロスアカウント…データ分析基盤を運用しているアカウントを経由することで、その権限を持っていないアカウントで間接的にデータにアクセスすること

メッセージキュー（messagequeue）とは、ストリーミングを利用するシステム間でデータ注28の受け渡しを仲介し、データを一時的に保持（キューイング）するミドルウェア。データをメッセージキューへ送信する側をプロデューサー、データをメッセージキューから取得する側をコンシューマーという。また、プロデューサーがデータをメッセージキューへ格納することをパブリッシュするという。コンシューマーが利用するるのはconsume、もしくはsubscribeするという。

SSoT
データのサイロ化…データがそれぞれのシステムで溜め込まれており、小さなデータ分析基盤のようなものを複数形成してしまうこと。こうなると本当のデータがどこにあるのかがわからなくなる。
Single Source of Truth…すべてのビジネス データを 1 か所に集約するという慣習で用いられる言葉です。このような情報源を持つことで、その企業の全員が、相互にアクセス可能なデータに基づいて重要なビジネス上の意思決定を行えるようになる。

SSoTの種類
フィジカルSSoT...物理的にデータを1箇所に集める仕組み。データを一つに統合するために、対向システムからデータを毎回ETLし、一つの場所に集めるのに労力がかかる。
ロジカルSSo…あたかも1箇所に集めたかのようにユーザーに見せる仕組み。データのインターフェースは統一されていますが、そのインターフェースから先のデータはAはオンプレミスだが、Bはクラウド上にあるといったデータの管理方法。

データクラウド…データクラウドは自分のデータ分析基盤に取り込むのではなく、クラウド上に保存されたデータを参照して利用するというもの。更新や管理の手間がかからないのがメリット。

タグ…メタデータの一種でデータに付与される属性、ステージングゾーンのデータにステージングとタグ付けするなど。

タグを使った論理ゾーン化によるゾーン管理
物理的なゾーンでデータを分けると、データサイズの大きいデータの行き来はかなりの時間がかかる。新たなレギュレーションの追加などで見せたくないデータが新たに出現した場合、対象データをクォレンディーゾーンへ即座に隔離することができない。
タグ付けによる論理ゾーン化…論理ゾーン化ではデータの位置は関係なく、データに対してタグという識別子を付与する。クォレンティーンゾーンのデータには「機密」というタグをつけるなど。論理ゾーン化のメリットはデータの移動がないこと

GA（Generally Available）パターン…データを取り込む際にセキュリティチェックなどの提携チェックの流れをデザインパターンとして表現したもの。特に機密データの取り込むを行うときに多く利用される。GAパターンでは一度データを全てクォレンティーンゾーンへ格納する。そこでセキュリティチェックなどを行い、機密情報が含まれている場合はデータはそのままクォレンティーンゾーンへ配置したままで、そうでない場合、以降のローゾーンへ配置を許可

プロビジョニングパターン…必要なときに必要なデータを仮に取り込むことを指すパターン。プロビジョニングパターンでは、まずデータはテンポラリーゾーンに配置されます。分析者はテンポラリーゾーンへ配置したデータと、すでにストレージレイヤーに存在するデータ（データレイク、データマート、データウェアハウス）と必要に応じて掛け合わせを行なって分析を行う。そのデータが見込みがあるとわかれば、パイプラインを通してフォーマルに取り込みを行う。

プロビジョニングパターンとGAパターンはそれぞれ、ユーザーがデータの出し入れを自由にできるのでデータが未チェックのままでデータ分析基盤に入り込む、チェックする人がボトルネックになる（その人がいなければできないし、ミスするかもしれない）という問題がある。
→これを解決するのがシステムによる自動チェック

テーブルの設定
RDBではテーブルの削除はそのまま実データを削除することにつながるが、ビッグデータの世界ではテーブルの定義と実データが明確に分離されている。
ロケーション…テーブルの定義を作成する際に実際のデータがどこにあるのかを指し示す場所
パーティション…データの格納場所を分割することができる機能です。1つのテーブルを日付の範囲などで、複数に行分割することができます。その時点でのスナップショットデータを残すことが可能であるという点。

データ欠損の原因の多くは人的オペレーションミスであるため、人はできる限り触れない方がいい。

バージョニング…データの更新ごとに古いバージョンをバックアップしておく機能

データのアクセス権限
データの制御をかける粒度はゾーン、データベース、テーブル、カラム/レコードの４つ。上に行くほど広く、下に行くほどアクセス制御の範囲が狭くなる。

One Size Fits All問題
組織内のユーザーが増えることによって、システムは肥大化傾向にあった。一つのクラスター上に幾つものアプリケーションを並べ管理する(One Size Fits All問題)だけでは障害児に大きな事故を引き起こしかねない。そもそもノードや名前空間を分けずに一つの環境にさまざまなアプリケーションを載せるという行為はエンジニアにとって危険すぎる選択である。一つが壊れることで全てに影響が出るため、それを最小限にできるように出カップリングという考えがデータ分析基盤にも出てきた。

デカップリングにより、障害児の影響の最小化、計算リソースの最適化といったことができるようになった。

データのライフサイクルマネジメント
データのサマリー化…データを削除せずに済む方法として、サマリー化がある。通常サマリー化は、パーティションをまとめることにより実現される。
コールドストレージへデータをアーカイブ…コールドストレージは使用頻度が低いデータを保存するためのストレージ。アーカイブとはすでに参照されなくなったデータを別の媒体に移して遠隔地へデータを移してしまうこと。コールドストレージは使用頻度が低いデータを保存するためのストレージである。データを保存するには低価格で行うことができる。
不要なデータを削除する…データウェアハウスやデータマートは積極的に削除すべき。

ハイブリッド構成の大きなメリット
ハイブリッド構成…オンプレミスやクラウドを組み合わせるハイブリッド。データ分析基盤から見たハイブリッド構成のメリットは、基本的にはデータを統合するコストを削減可能であること。
データを統合するコストの削減を実現可能な仕組み
・データバーチャライゼーション…構造化、半構造化、非構造化データベースを技術的詳細の考慮不要で統合する仮想的なレイヤー。これを使うことによって、そもそもデータを取り込まないという選択肢を取ることも可能。データヴァーチャライゼーションでは、データを取り込まずとも外部のデータソースを指定しテーブルを作成します。作成したテーブルに対してSQLを通して分析が可能です。ただし、管理コストが大きくなってしまうことには注意。
・データクラウド

ハイブリッド構成のデメリット
データの所在、オーナーが不明になりやすい
管理対象の増加によるコスト増大や障害復旧の複雑化

コレクティングレイヤーにおける技術スタック
・バッチ
Embulk…HA環境におけるLinuxサーバー上で利用されることの多い技術で、プラグインと併用したRDBやCSVなどの高速な取り込みを実現する。他にも、プラグインが選択下の出たようなデータの取り込みを実現可能、かつ着脱が容易で、再実行時に同じ結果が保証される冪等性（実行するたびに同じ結果が得られる）がある。

HA構成とは、可用性が高められたシステム構成のことで、HAはHigh Availabilityの略で、「可用性が高い」という意味です。 可用性とは、システムが継続して稼働できる能力のことで、つまりHA構成とは「高可用性を実現するための構成」という意味になります。

Dump…MySQLやPostgreSQL、クラウドストレージからの取り込みで活躍するツール。HA環境におけるLinuxサーバー上で利用されることが多い技術。

・ストリーミング
Splunk…
Fluented…
メッセージキュー…コレクティングレイヤーにおいて、データの受付口を担当する。メッセージキューとは、プロデューサー（producer）とコンシューマー（consumer）の2つの間を取り持つプロダクトのことです。例として、分散メッセージキューであるKafkaがある。
Apache Kafka
Amazon Kinesis
Google Cloud Pub/Sub

プロビジョニング
プロビジョニングは「仮の」という意味で、その名のとおり一時的にデータをデータ分析基盤に配置させる取り込み方法です。このプロビジョニングの作業はIT担当のデータエンジニアは関与せず、データアナリストとデータオーナーのマッチングサービスのように利用される。プロビジョニングは好きな時に好きなようにデータを取り込んでデータを分析することを目的としたデータの取り込み方法であるため、必ずしも自動化される必要はない。しかし、簡単にデータが取り込めるようにWeb画面で操作できるなど、必要に応じてデータパイプラインの整備をおこなっておくと良い。

プロセシングレイヤーの技術スタック
・バッチ
行われること一覧
・バッチ処理におけるETL
・データ品質測定
・メタデータ算出
・暗号化
Apache Spark…バッチ処理に使われるプロタクトの筆頭として、Apache Sparkが挙げられる。Sparkはバッチ処理からストリーミング処理まで一つで完結することができる単一コンピューティングエンジン。Python, Java, R, Scalaで記述をすることができ、データ系の開発におけるスタンダードなツールとなっている。SQL、ストリミーンぐ、機械学習ライブラリ、DataFrameを使用することができ、それによって、ETL、データラングリング、メタデータの算出、暗号化、データ品質算出などを行うことができる。

Apache Hive…Facebookによって開発されたSQLライクな分析体験を提供するプロダクト、Apache Hiveは事前に定義したテーブリに対してSQLを実行して分析することが可能で、利用されるクエリーはHive SQLと呼ばれている。Apache Hiveではスキーマオンライト方式で事前にテーブルの定義を行い、その定義に従って、HiveSQLを実行する方式をとっている。なぜ動いているのかよくわからない複雑怪奇なプログラムは黒魔術Hiveと呼ばれる。

・ストリーミング
ストリーミング処理がバッチ処理と異なるのはそのデータの処理が一件ずつもしくは数分単位での小さなまとまりのデータで処理を行うこと。データが順次流れてくるという特性からシステムを一度停止してのメンテナンスを行いづらく、システムを稼働させたまま新規アプリケーションをデプロイするのがほとんど。

Spark Streaming … ストリーミングETLの処理でよく使われる。メッセージキューと組み合わせて頻繁に利用される。コレクティングレイヤーのメッセージキューよりSpark Streamingを用いてデータを取得し、取得したデータに対してSpark StreamingでETLを行う。（その時に一意を表現する識別IDなどを導入する）。この作業はSpark Streamingだけでも成り立つが、メッセージキューを挟むことによって、デカップリングが可能となり、プロデューサー側による影響とコンシューマー側の影響を分離し、一方の影響がもう一方に出ないようにすることでより安定したステムを構築することが可能となる。

・データ転送
やり方一覧
・AWS CLIやGoogle CLIなどのコマンドラインやSDKを利用する。コマンドラインやSDKは各クラウドベンダーから提供されているクラウドのリソースを操作可能なツールである。
・Rcloneなどのデータを同期するツールを利用する。Rcloneとはマルチクラウドに対応したデータの同期ツールである。AWS CLIやGoogle CLIとは違い、プログラムを書くことなく設定ファイルを記載することでファイルの同期が可能となる。
・Google Strage Transfer Serviceなどのデータを転送するためのマネージドサービスを利用する。Google以外のサービスからデータを取り込むのに便利なツールである。

プロダクトの連携時によく使われるファイルのバッチ転送方法はAmazon S3→RedShiftなどのプロダクト間の連携は、各ベンダーが提供しているコマンドベースで実行される。
クラウド間で利用される転送方法としてDistCp(Distributed copy)による転送も存在する。DistCpとはHadoopにおけるデータ転送方法で、Hadoop環境から別の環境へデータ転送を行う際はよく利用される。

ストリーミング処理における転送方法
ストリーミングにおけるデータ転送は、コレクティングレイヤー（メッセージキュー）から受け取ったデータに対して必要なELTをプロセシングレイヤーで施し、その生成結果であるデータを再度別のメッセージキューに引き渡しを行います。後者のメッセージキューはデータ分析基盤の中に存在するメッセージキューとは限りません。別アカウント（クロスアカウント）のメッセージキューである可能性もあります。

ストリーミングデータの送受信で気をつけるべき点
ストリーミングデータの送受信で気をつけるべきは、データの送信遅延とデータ重複である。
例えば内部処理による一時的な速度減に対処するといったパターン。データの遅延に関しては長くても5分から10分のウォーターマーク（識別情報、透かし）を設けるのが普通。ウォーターマークとは、Dataflow でウィンドウのすべてのデータが必要になるしきい値を表します。新しく受信したデータのタイムスタンプがウォーターマークより古い場合、データは遅延データとみなされます。

データ重複のよくある３パターン
①同じデータが2回送信される場合
②同じデータを2回メッセージキューから読み取ってしまう場合
③クラスターないのノードが同時にデータをメッセージキューから読み取りにいくケース

これらの重複に対処するには、2種類のUUID(Universally Unique identifier)が通例使われる。
・一つはプロダクト（GCPやKafka）が受け付けたデータに対して付与するUUID。
・もう一つは送信側が付与する独自UUID(ハッシュなど、一意が保てる物を送信時に付与する)

しかし、ストリーミングの場合は、ウィンドウ（一定の間隔で時間を区切り処理を行うこと）内での重複削除のみ有効で、ウィンドウ間での重複処理には対応していない。よって、ウィンドウ間の重複削除に対応するにはストレージなどに出力した後に、データラングリングなどを通した重複除外処理で重複削除をすることが必要。

データパイプラインとワークフローエンジン
データパイプラインとは全てのデータが発生してからユーザーに届くまでの通り道のこと。データパイプラインオーケストレーションとは、データパイプラインの指揮者という意味で、コレクティングレイヤーからストレージレイヤーへデータを格納するまでの流れを統括すること。

ワークフローエンジンは特化型と汎用型の2種類に分けられる。

データ分析基盤向けのワークフロー選択時のポイント
・ETL処理て定義ファイルベースで記載できること（CI/CDが行いやすくなる。CI=Continuous Integration、CD=Continuous Delivery）
・再実行時に同じ結果が保証される冪等性があること
・途中からやり直しができる機能を有していること
よく使われるワークフロー型エンジン
・DigDag…データパイプラインをyml形式で記載可能。Embulkと親和性がよく、データエンジニアリングの世界では多く使われている。いいデータパイプラインを作成するための必要項目を含んでいますが、マネージドサービスが存在しないため、サーバーの構築や管理は自分で行う必要がある。
・Apache Airflow…Pythonで定義可能なワークフローエンジン。Digdogに比べると自由度が出る反面、よりエンジニア向けの製品に分類される。AirflowはマネージドサービスがAWSやGCPからそれぞれ発表されている。
・Rundeck…GUIベースのワークフローエンジンで、スケジューラーにも分類される。GUI以外でも設定可能なので、エンジニア以外の方でも修正可能な場合がある。CI/CDはやりづらい。

ストレージレイヤーの技術スタック
データを扱う際のフォーマットとして、CSVやJSO Nがあるが、データレイクでは利用できるがその他では利用を避けた方がいい。理由はこれらのデータフォーマットでは、何億のレコードを同時に処理するデータ分析基盤において、分散処理が適用できず、一つのノードで処理することになるため。

行志向フォーマット…IoTやWebの回遊ログなどの収集時に発生する高速なレコード処理に適している。
列志向フォーマット…データ分析利用に特化している。分析をする際に利用されるGroupy by 構文や、COUNTなどの集計に特化しているフォーマットと言える。クラウド環境でデータ分析基盤を構築している場合mSQLの実行はスキャンしたデータ量に応じて料金が課金されることが多いため、列志向フォーマットを利用しないと金成の金額を支払うことになる。行志向フォーマットは行方向にデータを保持するため、ストリーミングのように秒単位でレコードが追加されていくような仕組みに向いている。

Parquet...もし迷ったらparquetを選択すれば安定してパフォーマンスを出すことができる。列志向フォーマットを持っており、カラムごとに圧縮が効くため効率よくデータをストアできる。そして最後に、多くのプロダクトがサポートしている。
Avro…Hadoopの生みの親である人によってプロジェクト化される。ストリーミングでのやり取りで効力を発揮するフォーマット。行志向であり、前方互換性、後方互換性、完全互換を持ち複数のシステム間で速度の違う開発を行うことが可能。スキーマエボリューションを提供する。Parquetに比べてJsonのようなリッチフォーマットを表現可能。Jsonに似ていますが、スキーマエボリューションを提供していない点や、定義不可能な型の構造定義が可能な点、およびスプリッタブルあるかどうかという点が違う。

スプリッタブル…一つのデータを複数のノードに分けて処理可能かどうかというということを示す指針。

スキーマエボリューション…後方互換や前方互換の機能を利用することによって、一方のシステムへ変更があったときでも、他のシステムの稼働を維持しつつ自システムの変更を行うことができる仕組み。

シリアライズ…複数の要素を一列に並べる操作や処理のこと。単にシリアライズといった場合には、プログラムの実行状態や複雑なデータ構造などを一つの文字列やバイト列で表現する「直列化」を指すことが多い。
デシリアライズ…シリアライズ操作により一つの文字列やバイト列に変換されたデータを、もとの複合的なデータ構造やオブジェクトの実行状態などに復元する処理や操作のこと。

データ分析基盤が扱う圧縮形式
スモールデータシステムであれば、ファイルはgz形式で保存されていることが多い。一方で、データ分析基盤ではgz以外にも用途に合わせたデータ分析基盤特有の圧縮形式が存在している。

・Snappy形式…さまざまなデータを利用する際に使われる圧縮形式。多くのプロダクトがSnappyの圧縮形式をサポートしていて、データ分析基盤の大半はこの圧縮軽視でOKと言っても過言ではない、圧縮自体は軽量で速度も良好、迷った場合はsnappyを検討するのがおすすめ

・Bz２形式…bz2は標準のLinuxマシンであれば標準のコマンドとして用意されており、手元のマシンでも使えるビッグデータ定番の圧縮形式、主にCSVやJSONなど、さまざまなシステムやツールがエクスポート可能なデータフォーマットに対して利用すると、効力を発揮する。

gz形式…さまざまなシステムで利用されている圧縮形式、bz2形式より早い速度で圧縮可能。Snappyよりも圧縮率が高く、容量削減にもなるが、Snappyよりもデータを処理する速度は遅い。

スプリッタブル…一つのデータを複数のノードに分けて処理可能かどうかというということを示す指針。スプリッタブル…一つのデータを複数のノードに分けて処理可能かどうかというということを示す指針。

圧縮形式とフォーマットの組み合わせ。

￼

CSVやJSONを扱うときはbz2を利用する。CSV、JSON以外のフォーマットでは、基本的に全ての組み合わせでスプリッタブルを実現することが可能。そのため圧縮率を高くしたいのであれば、gz形式を、速度を求めつつ圧縮したいのであればSnappy形式を選択するといい。

データストレージの種類
・クラウドストレージ（Amazon S3やGoogle Cloud Strageなど）
クラウド上で提供されるストレージ。安価に提供されており、データ分析基盤の保有する大量のデータを保持することに向いている。容量は無制限でディスクの交換も不要で、データ分析基盤のために生まれたようなストレージ。

・プロダクトストレージ（Amazon RedshiftやGoogle BigQueryなどのMPPDB）
特定のプロダクト内部にデータを保存する方法。Amazon RedshiftやGoogle Big Query、そしてSnowflakeといったベンダーに依存していないプラットフォーム。プロダクトストレージはクラウドストレージに配置してあるデータを自身のプロダクト内に取り込み、処理を行う。このプロダクトストレージがサポートしているプラグインを利用しでデータの利用を進めていくケースもある。プラグインとしてはSQLだけでなく、ノートブック環境であるJupyterを通してBigQueryなどに接続し、SQLを実行してそのまま結果を可視化するといったことも可能。ただし、プロダクトストレージは特定のベンダーにロックインされる可能性もある。

・オンプレミスにおけるストレージ(オンプレミスで利用されるSSD(Solid-state drive))
Hadoopなどで利用される。新規利用はいまいち

ストレージの種類
・ファイルストレージ…普段使いのパソコンでもお馴染みの特定のパスに保存するストレージ
・オブジェクトストレージ…データを<key>:<value>形式で保存するストレージ。Amazon S3やGoogle Cloud Strageなど
・ブロックストレージ…HadoopにおけるHDFSと呼ばれるファイルシステムで利用されている、ファイルをブロック単位で分割して保存するストレージのこと

データストレージへのデータ配置
オンプレミスでもクラウドでも、１ファイルの大きさには注意すべき。ビッグデータを扱うプロダクトでは1KBが1億個存在しているよりも、100MBが100個の方が処理が得意。
オンプレミスのシステムではファイルが多すぎると管理用のメモリー消費が大きくなります。クラウドでは計算能力とストレージはでカップリングされており、オンプレミスの時に発生していたメモリーの消費は解決されているが、ファイルを読み込むためのオーバーヘッドが大きい問題は残る。さらに、小さすぎるファイルはSQLの実行速度を低下させたり、オブジェクトストレージへのリクエストへのリクエスト過多でデータが参照しにくくなったり、データのコピーの速度低下を引き起こす。スプリッタブルではないファイルのファイルサイズは1-2GBくらい、スプリッタブルなものは2-4GBくらいが理想。

・データスキューネス…ビッグデータを扱うプロダクトでは１KBと1GBの２ファイルを処理するよりも500.1MBと500MBのファイル２つを処理する方が得意であり、ファイルサイズは近づけるべき。
分散処理ではノードが処理するファイルを分配するが、その際に極端にファイルサイズに差があると、一方のノードが動き続ける一方で、もう一方のノードは処理が終わり、暇になるという事態が起こる。


アクセスレイヤーの技術スタック
・BIツール
BIツールの主要な機能は「可視化である」。

可視化のみ担当するBIツール
Amazon QuickSightがある。ダッシュボードとは複数の情報をひとまとめにした表示やそのためのツール。

SQLの実行と可視化を担当するBIツール
多くのBIツールは可視化とSQLの実行がユーザーで自由に実行可能。それらはユーザー自身がSQLを通してデータを可視化して利用できる。
代表的なBIツール一覧
・Redash
・Metabase
・Power BI
・Tableau
・Looker

BIツールで提供されるSQLの種類
・Presto(Trino)…BIツールを利用してよく実行されるSQLのタイプ。PrestoはJavaで記述されたインメモリー型のクエリーエンジンであり、Prestoのクエリーエンジンで実行されるSQLのことをPrestoSQLと呼ぶ。比較対象として挙げられるHiveとの違いは、Hiveは途中の計算結果をディスクに吐き出しながら処理を進めるのに対し、Prestoはメモリー内のみで処理を完結させる。ディスクのI/o(input/output)よりもメモリーのI/Oの方がより高速に処理することが可能なため、PrestoはHiveに比べて高速に動作する。一方でメモリーに収まりきらなかったり、大人数での並列利用を想定した時にはHiveやSparkの方が素早く処理できる。この特性からアドホック分析で頻繁に使われる傾向にある。

・PostgreSQL
ビッグデータでPostgreSQLを利用することは多くないが、ベースにしているプロダクトは多くある。分析でよく用いるWIndow関数が豊富。PostgreSQLをベースとしたプロダクトではPostgreSQLの文法がそのまま利用可能である。例えば、Amazon RedShiftはPostgre SQLベースで構成されている。

・Notebook
少しのエンジニアリング力が必要になるものの、自分自身で拡張可能なノートブックを通してスキーマオンリード方式を利用して直接ストレージレイヤーにアクセスを行うことも可能です。ノートブックはデータラングリングやエンジニアリングの重要ツールです。

・ストレージアクセス


・API
データ分析基盤におけるAPIの意味は、データそのものを提供するという意味合いではあまりない。理由はAPIで返せるほどデータが小さくないから。そのためデータ分析基盤において、APIの用途は以下の二つに大別される。→処理を行うためのクラスターを起動したり、JpyterNotebookを起動したりすることを指す。
・ETL処理やその他のデータ分析基盤に対する処理をラップする
・メタデータの提供

インターフェースの数
スモールデータシステム（webシステム）などであれば、GUIとAPIの2種類のインターフェースがあれば事足りる。しかし、データ分析基盤のインターフェースは多岐にわたっており、APIやBIツール、ストレージアクセス、GUI(メタデータやデータ品質提供)、メッセージキューといったさまざまなインターフェースがある。

ユーザーの認証と制御の歴史
Chmod/chown
データ分析基盤で初期に利用されていたものはGeneral Access Controlと呼ばれるユーザーとビット管理による直接権限をデータに設定する方式でした。しかし、データが大きくなるにつれて問題が出てきました。その問題とは権限を変更するためのコマンド実行が終わらなくなったこと。

IAM
データに直接権限を設定するのではなく、ユーザー側での制御を試みるようになる。新たな権限の設定方法であるIAM(Identity and access management)が登場する。IAMは認証/認可の役割を持っていて人やシステムに対してロールを与える。認証とは相手が誰かを確認することで、認可とはリソースアクセスや実行の権限を与えること。例えばwebシステムにログインするときのユーザーとパスワードに入力する行為を「認証」読んでいて、ログイン後に操作可能な行動範囲を規定することを「認可」と呼んでいる。IAMはデータだけでなく、アクセスレイヤーにおける特定のインターフェースを使わせないなどといった制御も可能です。ビッグデータシステムでは、データとメタデータが分かれている。例 AWS IAM, Cloud IAM

タグによる管理
現在クラウドで利用されているIAMは、タグベースとロールベースの2種類をどちらもサポートしている。データ管理においては、IAM単体ではデータの移動という少し足りない部分がある。大量のデータをA地点からB地点に移すことはコストのかかることです。そこで、タグと組み合わせることでデータのタグを書き換えるだけで元のデータに一切の変更を加えずに（移動せずに）ユーザー権限を変更することができるようになった。

メタデータ
メタデータを提供する方法としては、API方式やGUIとしてユーザーに提供する方法がある。

メタデータ…データを表すデータ。データの所在位置、フォーマットの形式などを示すデータの設計書となる役割を持っている。ストレージレイヤーにおけるメタデータストアに格納されている。メタデータストアは<key>:<value> の形で保存さてている。

メタデータを整備してユーザーに提供すべき理由
①疑問点の解消につながる…このデータは何を示しているのかなど、データを効率よく見つける環境の整備は重要
②データに対するドメイン知識のギャップを緩和できる…車内特有のドメイン知識のギャップを緩和できる。
③データを利用するシステムや人の動きを統一する…データ分析基盤のユーザーが一律参照することが可能な指標をAPIやGUIで提供して、ユーザーはその指標を利用して動いていく句ことで正しくないデータ利用による機会損失を減らすことができる。
④非同期にデータを利用する状況を作る…データがいつごろ生成されるのかなど、メタデータとしての生成時間やテーブル定義を全体に公開してデータ分析基盤の管理者とユーザー間の同期する時間を極力減らすことに役立つ
⑤アクセス権限に縛られずデータを見つけるヒントになる…アクセス権限がないからデータを見つけることができないという状況は可能な限り避けるべきであり、データを普段見せることはできない場合でも、メタデータを提供することで、ユーザーがアクセス権限を機にすることなく想像を膨らませることができる。

・ビジネスメタデータ…テーブルやデータベースの特性を示す
業務に関するルールや、テーブル定義を表現するメタデータであり、ドメイン知識などと呼ばれることもある。業務ルールなどの形式知や人の頭に隠れた暗黙知を表現する部分になるため、エンジニアだけでなく、データオーナーやアナリストなど、データ分析基盤に参画するすべてのユーザーが積極的に参加し、ビジネスメタデータを構築していくことが好ましい。
ビジネスメタデータの例
・テーブル定義
・データの意味の説明、関連性
・組織内で使われる専門用語の意味や定義
・ドメイン知識

・テクニカルメタデータ…データに対する技術的詳細を示す
以下のようなデータに対する技術的詳細を明かす
・テーブルの抽出条件
・リネージュ（データの紐づき）やプロバナンス（データの生まれや期限）
・テーブルのフォーマットタイプ
・テーブルのロケーション
・ETLの完了時間
・テーブルの生成予定時間

・オペレーションメタデータ…システムの運用やデータの移動過程などで生成される。
データの処理とアクセスの詳細を表すメタデータのこと。データに対して5w1hと表現するためのデータと考えてもいい。例えば誰が、Aのテーブルにいつどのような方法で何に何回アクセスしたのかという情報やいつ、どのような理由や方法でテーブルを作成したのかという情報を表現するメタデータがオペレーショナルメタデータである。
以下のような情報を提供する。
・テーブルのステータス（In-Service:当日のワークフローが完了した, TimeLineness Violation:利用はできるが普段よりもデータ生成が遅延した, Error:当日のワークフローでエラー発生, Investigation:原因調査中）
・メタデータの更新日時
・１ファイルのデータサイズ

データプロファイリング
データプロファイリング…データを調査してデータの特性からデータそのものを推論することです。データに対する事実のを提供し、ユーザーがデータをSQLにてクエリーせずともデータがどこかを知ることができるようにするのがデータプロファイリングの目的。データプロファイリングをした結果、メタデータストアに保存しAPIやGUIなどのインターフェースを通してユーザーがデータの進捗状況を確認する。簡潔にはデータの状態を知ること。

データプロファイリングの結果を表現する方法は大別すると以下のようになる。テーブル（もしくはカラム、データベース、データ分析基盤全体）単位で指定したルールを守っている、守っていないを
１.YES/NOで表現する
２.数値で表現する

表現した値はメタデータストアに格納し、テーブル定義（ビジネスメタデータ）と紐付けてGUIで表現することによって、視覚的にどの値がどのような状態であるのかが分かりやすくなる。ルールはデータ分析の管理者が作成する。その際に、あまりスモールデータシステムや対向システムに合わせすぎないこと。都度スモールデータしシステムの都合と合わせていたのでは、一向にデータ分析基盤の統合が進まない。そもそもスモールデータシステムの細かな型はビッグデータシステムではサポートしていない場合もある。

データプロファイリングは小さく始めることが可能なカラムレベルでシステムへの適用を始めてみることがおすすめ。データベースやテーブルレベルでの表現は数値で表現する場合に数値が合算されたりと意味をなさない数値になってしまうことが多いから。

データプロファイリングの各項目とプロファイリング結果のメタデータを通した表現方法
・カーディナリティ(cardinality)…データがどれくらいバラけているのかを表した指標。性別は種類が少ないため、カーディなリティが少なくなるが、ipアドレスやIDはカーディナリティが高くなる。
・セレクティビリティ(selectibility)…ユニークさを表現する。１ならそのカラムはユニークである。どの対象のカラムを条件指定した時にレコードが何件返却されるかという指標になる。
・デンシティNull…nullの密度を示す指標。Nullが多いということはそれだけでデータから得られる情報が少ないということになる。
・コンシステンシー(consistency)…データの間に一貫性があるかどうかを表現する項目。
・リファレンシャルインテグリティ…データにはお互いに結合できるのかとは参照整合性があるかどうかを示す指標。
・コンプリートネス(completeness)…特定のレコードに必要な情報を全て含んでいるかどうかを示す指標。仮にデータがnullばかりであれば、あまり利用する価値がないかも知れないし、そもそもデータが正しく転送されているかどうかがわからない。そこで、その状態をより可視化するために、コンプリートネスを利用する。メタデータとして表現する場合は全ての値が揃っているレコードがXX %、そうでないレコードがYY %のように表現すると完結に表現することができる。
・データ型…データ分析基盤で制定したデータの型のルールに沿っているかを表す指標。年齢であれば数値であることが好ましいし、住所であれば文字列が好ましい。
・レンジ(range)…値が特定の範囲に収まっているかどうかを表す項目。年齢であれば、０〜125ほどの範囲になることが想定される。このように値の上限や加減が決まっている時に、そのルールが守られているかどうかを確認するのに有効です。
・フォーマット(format)…郵便番号や電話番号など、正規表現として表現可能なルールに従ってるかどうかを表すメタデータ。
・フォーマットフリクエンシー（Format Frequency）...形式のパターンごとの出現頻度を表現するメタデータ。複数のフォーマットが混合する場合に有効。
・データ冗長性(data redundancy)…データの冗長性を示す指標。データ分析基盤におけるSSoTの進行具合を示す指標であると言える。数値が高ければデータは様々なところに点在しているし、数値が低ければデータが1箇所に集まっているということになる。データ分析基盤においてデータのコピーはなるべく避けるべき。一度コピーを作り出すとコピーのコピーが発生し、いつの間にかデータのサイロ化を引き起こす可能性がある。.
・バリディティレベル(validity level)…対象のデータにおいて有効と判定可能なデータがどれくらい存在しているかを表す指標。例えば、ストリーミング処理にてUTF-8ではないデータがどれくらい存在するかを表現する場合、プロセシングレイヤーで対象データをUTF-8へバイト玄関できない場合は、無効である旨のデータをプロセシングレイヤーで付与することにより、データをそのままリアルタイムでvitality levelの割合を見ることも可能。さらに、一度ストレージに格納した後にバッチにてETLして有効でないデータはどれくらいなのかを表現することも可能。
・アクセス頻度（frequency access）…どれだけデータにアクセスされているのかの頻度を表現するメタデータ。データはストレージレイヤーやメッセージキューにデータを配置してからが勝負。

データカタログ
データカタログ…手元にないデータを登録し、取り寄せるためのデータのためのカタログである。そこまで作り込む必要はないが、どのようなデータが存在していて、データを取り込むための方法が選択可能（embulkなのかdumpなのか）で、プロビジョニングするとしたら期限はいつまで利用可能なのかといった情報を載せることで、データの利用を促進しています。
3種類の認知
・ストレージレイヤーに保存されていて、ユーザーが認知しているデータ…
・ストレージレイヤーには保存されていないが、誰かが認知しているデータ
・誰も認知していないデータ

データアーキテクチャー
データアーキテクチャ…コレクティングレイヤーからアクセスレイヤーまでの流れを示した設計書のこと。データの生成経路のトラッキングやドメイン知識を組織内に伝達する用途で利用する。設計を言語化する手段として、リネージュ(lineage)とプロバナンス(provanance)がある。リネージュとプロバナンスはビジネスメタデータ、テクニカルメタデータ、おペレーショナルメタデータという３つの側面を持ち合わせています。

データアーキテクチャやデータカタログの機能などに対応しているプロダクト一覧
・Atlan
・data.world
・Quilt

リネージュ(Lineage)…テーブルの紐付き。
作成するメリット
・データ生成の方法が記載されているので、利用している技術スタックの見通しが良くなる...データ分析基盤は様々なデータソースからのデータが集う場所です。よって、様々なデータソースを組み合わせてテーブルを生成することが想定される。
・障害児のトラッキングが行いやすくなる…仮にリネージュを整理していない場合は、様々なデータやテーブルが生成されていくデータ分析基盤において、何かの障害やインシデントが発生したときに、どこまでが影響範囲なのかということが即座にわからない。
・オーナーの明確化が可能…何かあった時にどのデータに対して疑問点など聞くことができる人を明確にする用途で使われる。
・データの設計書を残せる…誰かにデータが利用されるという前提に立ち、データの設計書としての役割を果たす。
・プロバナンスやメタデータと統合する…プロバナンスや他のメタデータと総合的に参照できるとさらに強力なツールとなる。

プロバナンス（provenance）...データの発生源の情報
データ分析基盤においては、スモールデータシステムの設計書の段階まで遡る。少なくともデータ分析基盤へデータを取り込む前は、スモールデータで生成されたデータがどのような条件下で作成されたのかがわからない。よって取り込みを行なったスモールデータシステムの設計書や更新条件などの明記がないと、そのシステムのドメイン知識が一向に蓄積されない。これも取り込み元の巣も０流データシステムを作成した人とデータ分析基盤の管理者が違うことに起因している。

データモデル…表現の粒度のこと。データアーキテクチャをどこまで細かく記載していったらいいのか。
カラムレベルでデータモデルの表現をしようとすると、一気に粒度が細かくなる。そのため、まずはテーブルレベルでの表現を目指すことをお勧めする。そこから外部一貫性（External Consistency）を意識しつつ、重要なテーブルからカラムレベルでの表現を行なっていくと、リネージュに関する開発も小さく始めることができる。

Hadoop
20年前に生まれた
大量のプロダクトの追加とコントリビュートが進むにつれ、Hadoopとエコシステムは肥大化し、インストールや設定、依存関係の把握も複雑化

デカップリングの登場
クラウドの登場のおかげで、計算能力とストレージを分離し、それぞれをほぼ無限にアウトスケールすることが可能になった。

Hdoop on Cloud…オンプレミスで活躍したHadoopだが、クラウドでも特にプロセシングレイヤーにおける技術として活躍している。日に数千を超えるETLやデータ品質チェックのジョブなどを処理するにはまだまだHadoopの活躍の場は残っている。

EMR,Dataproc
EMR…Hadoopが使用可能なAmazonのプラットフォーム。メイン要素としてマスターノードとノードで構成されており、マスターノードはノードを制御する。日に数万jobのバッチ処理を行う場合などは、後続で紹介するKubernetesと比べると、より安価に安定的に処理可能なEMRが採用されるケースが多い。

Dataproc…Hadoopが使用可能なGoogleのプラットフォーム
エコシステムを利用すれば、ビッグデータにおけるバッチ処理からストリーミングまでほぼ全ての処理が提供可能。

Kubernetes…Linuxコンテナの管理運用を自動化するクラスター（プラットフォーム）で、コンテナを管理運用するオーケストレーション機能を提供。メイン要素としてコントローラー、ノード、podによって構成されており、コントローラーはノードを制御し、タスクを割り当て管理を行います。ノードはコントローラーから割り当てられたタスクをpodに割り当て、実行指揮する機能です。

MPPDB(Massively parallel processing database)…データベースの一種。構造はEMRヤKubernetesと同じで一つのリーダー（マスター）がノード集合体を指揮するという形をとっている。データベースの一種のため、表形式の構造化したデータの処理を主に担当している。MPPDBをクラスターとして選択すると、プロセシングレイヤー、アクセスレイヤーの全てを賄うことが可能です。そのため、分析の環境をシンプルにするためにも、MPPDBに全てのデータを集めることも選択肢の一つ。


DIKWモデル
DIKWモデルでは、データのステージを「Data」「Information」「Knowledge」「Wisdom」として定義しているこれらの頭文字をとってDIKWモデルと呼ばれている。

Data…DIKWモデルにおいて最下層に位置するのがdata。dataに該当するのはローデータに当たり、データ分析基盤へ取り込みをした時点でほとんど手が加えられていない状態を指す。

Information… Dataだけでは物事を俯瞰して捉えることが難しいため、DataをInformationへと整備する必要がある。DataからInformationへの変換はデータの分類や整理を行います。分類にはLATCHと呼ばれる分類の種別を使って行う。LATCHとは、以下の５つの分類方法の頭文字をとったものです。
・Location(場所）...場所ごとの分類
・Alphabet(アルファベット)…辞書順
・Time(時間)…時間別に分類することを指す
・Category(カテゴリー)…カテゴリーごとの分類
・Hierarchy(階層)…階層に分類すること

Knowledge…Informationを精査し、そこから得られた事実に基づくパターンや関係（規則やルール）のことを表します。データを探ることでわかる事実。データの利用でよく使用されるBIツールで可視化したダッシュボードはまさに事実を表現したknowledgeである。

Wisdom…Knowledgeだけでは、データから得た事実がわかるだけであり、そこから物事を改善しようというデータの活用に一歩及ばずである。ここで出てくるのがwisdomであり、日本語で言えば、データをもとにした賢明な選択をサポートするもののこと。

データマートとデータウェアハウスの違い…データマートはより特定のユーザー向けに作成および利用される一方で、データウェアハウスはデータマートよりも多くの人に向けたものというのが現在の立ち位置。実際にはデータマートとデータウェアハウスないのデータベースやテーブルは特定のユーザーやチーム単位で用意することが多い。そのため、データベースやテーブルを作るなら予め定めたルールに従って、データエンジニアリングとしてIT部門が肩代わりするのではなく、セルフサービスモデルをとったほうが効率がいい。

データ分析基盤においては、スキーマ設計において非正規化が推奨されているなど、RDBと設計思想が異なる部分がある。

スタースキーマ(star schema)…長くスキーマ整理の基本として紹介されるスキーマの設計方法。ファクトテーブルと呼ばれるテーブルを中心として、そのテーブルに紐づくディメンション（次元）テーブルを配置したスキーマ設計のこと。主に小さなデータマートで利用される。ファクトテーブルとディメンションテーブルが「1:n」で紐づくように設計する。ディメンションーブルにはあまり更新がない一方で、ファクトテーブルは頻繁に更新が発生し、データが増え続けていく。
ファクトテーブル…本体(数値や名前のkey)
ディメンションテーブル…本体に対する付加情報をまとめたもの。（実際の名前や数値など(value)）
スキーマ設計には大事な一面がある一方で、完璧なものを作ろうとして作業が進まなくなってしまわないように注意が必要。ただし、ビッグデータの世界になるとパーティションを分割するなどして更新したデータを都度保存しておくことが多いため、スタースキーマはあまり使われなくなった。

非正規化
データ分析基盤では正規化の逆で、非正規化を行いテーブルの設計を行なっていきます。正規化処理ではデータは一貫性と効率性を求めるため、データをエンティティごとにテーブルを作成し保存している。一方で、非正規化ではエンティティごとの分割は行わずに一つのテーブルとして保存していく。データ分析基盤で非正規化が一般的となっている理由について、正規化における利点は、読み込む量のデータを少なくすることで、検索や更新のパフォーマンスが高まるといった汎用性アップが挙げられる。一方で、データ分析基盤のデータはレコード単位で更新されることはあまりなく、特定の日付を丸ごと更新するといった洗い替えをメインに行うためです。

また、データ分析基盤のデータはデータが大きくなることが多いため、JOINを何個も結合していると、それだけでパフォーマンスに影響が出ることもある。

データ分析基盤ができるデータマート作成のサポート
・データ利用のためのインターフェースを複数用意すること
・メタデータのためのシステムや仕組みを整備指定いくこと
・中間テーブルの作成や、データマート、データウェアは宇津におけるスキーマ設計についてアドバイスすること

データマート（中間テーブル）を作成する際の注意点
・アクセスログを使って中間テーブルを生成する
アクセスログよりアクセス頻度の多いテーブルを抽出する→アクセスの多かったテーブルを利用してクエリーのExplain機能を使って機械的に結合しているテーブルを抽出する。→出現頻度の多いテーブルを抽出し、そのテーブルを含むSQLに対して分析を行い、スタースキーマや非正規化を用いて中間テーブルを作成する
・データアナリストとコミュニケーションを取りながらデータを生成する方法。データアナリストが記載したSQLを、データエンジニアがチューニングおよびスキーマ設計することによって進め、本番稼働させる。効果がある反面属人化、およびデータマートの効果範囲が特定のユーザーに限定される可能性のある諸刃の剣
→Viewによる仮想的な中間テーブルを作成するのがお勧め！余計なコストなく圧倒的速度でPDCAサイクルを回すことができる。

理想的なデータマートを作成してもらうためにできること
・定量的なデータを集める。→アクセス頻度を調べる。アクセスがそもそもない、アクセスの回数が減ってきているようなデータマートは不要になってきていると判断する。
・事前のルール作り→データマートの停止の条件を定めておくこと。データマートは基本的に特定の用途向けのため、大量に作成された後、用途を満たさなくなった場合などに大量に廃棄される。それ以降は多くの場合で使う機会がなくなるため、即座に停められるよう条件を定めておくこと。
・メタデータの活用
→オペレーショナルメタデータの利用
削除する予定のテーブルにつけておき、その三日後にテーブル定義のみが消え、1週間後にデータが消えるといった仕組み。
→ビジネスメタデータとしてのデータマートのテーブル定義を管理
データマートのテーブル定義をビジネスメタデータとして管理することで、いざ他のユーザーが既存のデータマートを利用しようと考えたときに、メタデータを参照し、即座に分析を開始することができる。


ストリーミング処理におけるデータマート
バッチ処理におけるデータマート作成の違い
・処理単位が小さい（１レコードから数分単位）
・ETL後のデータはストレージレイヤーではなく主にメッセージキューに格納される。
・ストリーミングシステムの停止はしづらい。
ストリーミングにおけるデータマートの流れ
①データソースからデータ発生
②コレクティングレイヤーで受付し、メッセージキューにパブリッシュ
③パブリッシュしたデータをプロセシングレイヤーでサブスクライブしデータ加工し、メッセージキューへパブリッシュする。（中間テーブルと捉えてもいいし、データマートとして捉えてもいい）
④③のデータをデータの利用者がサブスクライブしデータを活用する。
※ここでパブリッシャー側とコンシューマー側でリリースタイミングを合わせて停止することは、パブリッシャー側のデバイスの数やコンシューマー側のシステムが多くなるにつれて実質不可能になる。
そこで登場するのがAvroフォーマット。前方互換性と後方互換性、完全互換を持ち、複数システム間での速度の違う開発を行うことが可能。スキーマエボリューションを提供できる。


データ品質管理の３原則
①防ぐ/予防(prevention)→40%
②見つける/検知(ディテクション)→40%
③修正する/修理（リペア/repair）→20%

論理削除…現在有効か否かを示すフィールドの値を変更することで削除したのと同じ扱いにする方式。実際のデータは削除しない。
物理削除…データを物理的に削除する。

データ品質の要素
・正確性(accuracy)
・完全性(completeness)
・一貫性(consistency)
・有効性(validity)
・適時性(timelineness)
・ユニーク性(uniqueness)

データの劣化の４つの原因
①データの往来…通信要件やデータ転送時
②データの変換…ビッグデータシステムとスモールデータシステムの守備範囲の違いやデータマートを含むETLによる変換、プロダクト連携時にデータは劣化しやすい。
③時間の経過…貨幣価値、既に終了してしまったサービスのデータ、当時は利用されていたが、アクセスがなくなったデータ
④人的要因

データ品質テストの流れ
・レベルの設定を行う…テーブル単位で行うのか、カラム単位で行うのかといったデータ品質を行う粒度の基準
・テストの実施を行う…特定のカラムを利用して別のカラムと整合性がとれているかどうかをチェックするif-then。四則演算の結果が想定通りになるかどうかをチェックするゼロコントロール。想定した割合にデータの件数や統計量が収まっているかどうかをテストするレイショーコントロール(ratio controlo).
External Consistencyを確認するテストの手順
・natural keyを指定する…分析をする際にメインとして使われるIDの識別子など
・結合可能と思われるテーブル間でのexternal consistencyをテストする。
↓
・結果の可視化を行う
・結果から改善の手立てを打つ


インサイドアウトとアウトサイドイン
インサイドアウト…データ不備を事前に防ぐ、データのエラーを見つけて修正する
アウトサイドイン….ユーザーからのデータ修正依頼

実質的に使えるデータ品質の数値表現
・タグクォリティ…カラムやテーブルに対してどれたけテストが行われているかを表す指標。データ品質のカバレッジともいう。
・キュレーションクォリティ…人のメタデータへの介入がどれくらい存在しているかを表す指標。

KGI/（CSF）/KPIを定義して課題設定する
KGI(Key Goal Indicator)…最終目標
↑
CSF(Critical Success Factor)…重要性高要因
↑
KPI(Key Performance Indicator)…最重要業績指標


Datalog…監視ツール。システム面でのKEPの達成度合いがどれくらいかを測定してくれる機能あり。

SLO(Service Level Objective)…サーバーやネットワーク、ストレージなどの各領域の稼働率、性能、可用性、セキュリティといった項目ごとに、守るべき数値を明言した目標/評価基準です。
・データ分析基盤の管理者の立場では、ユーザーに対して数値的な目標を共有し開発における優先順位を決めやすくなる。
・ステークホルダーからの過剰な要求（事実上達成不可能な品質要求など）を回避することができる。

各レイヤーにおけるSLO
コレクティングレイヤー…データをもれなく収集できているか。
プロセシングレイヤー…エラー数が一定率以下
アクセスレイヤーにおけるSLO…APIとして提供しているプロダクトのレスポンス応答率、メタデータを提供しているGUI画面の稼働率など

VSM(Value Stream Mapping)…昨日要求が発生してから要求が顧客に届くまでの流れのこと


SQL基礎講座
スキーマ(schema)….型情報や名前のことで、たとえば「12」というただの数字に対して、「age」という名前と「Integer」という型情報を与える。
トランザクション（transaction）...複数のSQL文によるデータ更新を1つの処理としてまとめてデータベースに反映（確定）させることです。
Explain…SQLがどのように判断してSQLを実行したかという情報で、インデックスを利用しているかどうかの情報も提供してくれます。
クエリーエンジン…SQLを動かすソフトウェアやミドルウェアのこと

RDBの世界では、データアーキテクトを行うときには「正規化」と呼ばれる考えを表形式のテーブル設計に適用し、その結果をER図として表現することが一般的です。

正規化（normalization）...、テーブル設計においてデータの重複をなくしたり、整理をすることによって効率的にデータを扱えるようにデータを保存することです。単純に言い換えると「ルールに従ったしっかりとした形になっているかどうか」を規定するものです。

「購入者」「よみがな」「購入日時」「購入品1」「購入品2」「店舗名」「店舗タイプ」
この中で「購入品１」「購入品２」が似たようなものになっている。このように、同じような値が複数回登場するものを非正規型といいます

第1正規化（firstnormalform）...主に横方向へ増え続けるカラムを整理していくことです。
第2正規化（secondnormalform）...主キー属性と従属関係にあるデータの分離となります。
第3正規化（thirdnormalform）...すべての非キー属性についても従属関係の分離を行っていく作業のことです。

正規化のメリットは汎用性の増大、デメリットは検索パフォーマンスの低下

ER(Entity and Relationship)図…正規化を通して増えてきたテーブルを効率良く管理するための手法
また、数の表現にはIE記法を用いています。IE（Informationengineering）記法では、・○➡0を表す・|➡1を表す・鳥の足（三本線の足のつま先、図A.11を参照）を用いて表現します。


## ビッグデータを支える技術メモ

・ubuntuはLinux上で使えるOS
・Multipass launch -n primary　以降の記述がいまいち意味不明だし、書いても機能しない
・-y gcc python3-dev python3-venvは意味不明
・venvとは、Pythonの標準の言語処理系が持つ機能の一つで、システム上にPythonが動作する仮想的な環境（virtual environment）を作り出すもの。同じシステム上に複数の独立した環境を構成して使い分けることができる。
・まずpipとpip3の違いを理解していなかった。（ubuntuの仮想環境だとPython2とPython3が両方入っている。詳しくは以下↓
https://www.bioerrorlog.work/entry/install-pip-pip3-ubuntu#pip%E3%81%A8pip3%E3%81%AF%E9%81%95%E3%81%86

・Is Home
→Is: command not found

・jupyter lab --ip 0.0.0.0 --no-browser --noteboook-dir=notebooks入力。
→エラーCommand 'jupyter' not found, but can be installed with:
sudo snap install jupyter       # version 1.0.0, or
sudo apt  install jupyter-core  # version 4.6.3-3
See 'snap info jupyter' for additional versions.

・再度jupyter lab --ip 0.0.0.0 --no-browser --noteboook-dir=notebooks入力
Error executing Jupyter command 'lab': [Errno 2] No such file or directory
→pip3 install jupyterlab→解決せず→https://github.com/jupyterlab/jupyterlab/issues/3921を参照
→解決せず→https://na1.tech/258/を参照→解決。sudo apt -y upgradeをしてなかったから？

サーバーは立ち上がるようになったものの、クロムやsafari上でアクセス拒否
→jupyter lab --generate-configで設定ファイルを見てみる→特に怪しいところなし→どうもできないのでmultipass stop primary、multipass delete primary 、multipass purgeでインスタンス自体を一度完全に削除→第7章最初からやり直し→同じところでエラーサーバーアクセス拒否→ primaryをlaunchするときにメモリとディスク容量のサイズを拡大して指定することで解決

・sparkではcoalesce()関数を使うと、分散処理の結果を1箇所に集めて1つのファイルにまとめることができる
・#デスクトップにコピー　!cp ./export/*.csv ~/Home/Desktop
・chmod でファイルの管理者情報を変更することができる。
・ssh username@hostnameでsshコマンドを使ってログインすることができる。ubuntuサーバーを立てる場合は、usernameは自動でubuntuとなり、ホストネームは自動的にAWSコンソールのPublic DNSに記載されている。
・sftp はファイルを送るためのコマンド
・sudo gpasswd -a <username> <group_name（今回の場合は”docker”）>でsudo権限じゃなくてもdockerコマンドがpermission deniedではなくなる。
・dockerファイルをtarファイルにするときは、docker save <image名> > <tarファイル名>というコマンドですることができる。
・tarファイルをdockerに戻すときは、docker load < <tarファイル名>で戻すことができる
・Linuxでは、Dockerは/var/lib/dockerに保存されている。

・AWSのマネジメントコンソールからEBSの拡張をするだけだと、no space lett on device というエラーが出る。
→ボリューム全体のサイズは拡張されていますが、ルートのパーティションのサイズは変わってません。よって、sudo growpart /dev/xvda1の拡張を行う。それでも治らず。→追加でファイルシステムの拡張を行う（参考文献→https://tech-note-meeting.com/2021/04/28/post-888/）→解決！
・ubuntuでユーザーを作りたいときはsudo add userで追加できる
192.168.64.2 


プロキシサーバーとは使用すると、ブラウザで直接Webサイトにアクセスせずに、プロキシサーバーにアクセス、プロキシサーバーが代わりに目的のサイトにアクセスしてデータを受け取り、ブラウザにデータを渡して表示させます。
  
  
  現在の進捗
  
<img width="1277" alt="スクリーンショット 2022-06-20 16 55 41" src="https://user-images.githubusercontent.com/57390063/174552967-e33ac254-b88a-4e7c-8ef5-368f2c1206e0.png">
  
  
  <img width="1277" alt="スクリーンショット 2022-06-20 16 54 50" src="https://user-images.githubusercontent.com/57390063/174553011-6dadff49-3b83-4c71-8e78-edfb5f14137f.png">






